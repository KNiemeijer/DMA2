---
title: "Data Mining Assignment 2"
output: html_notebook
---

Load the text files into memory
```{r}
library(tm)
reviews.decep <- VCorpus(DirSource("C:/users/koenn/Google Drive/DM/Practical/DMA2/Reviews/deceptive", encoding="UTF-8", recursive=TRUE))
reviews.truth <- VCorpus(DirSource("C:/users/koenn/Google Drive/DM/Practical/DMA2/Reviews/truthful", encoding="UTF-8", recursive=TRUE))
reviews.all <- c(reviews.decep, reviews.truth)
labels <- c(rep(0,400),rep(1,400))
```

Preprocess the text corpus and create a document-term matrix
```{r}
# Clean all reviews
reviews.all <- tm_map(reviews.all,removePunctuation)
reviews.all <- tm_map(reviews.all,content_transformer(tolower))
reviews.all <- tm_map(reviews.all, removeWords,
stopwords("english"))
reviews.all <- tm_map(reviews.all,removeNumbers)
reviews.all <- tm_map(reviews.all,stripWhitespace)
reviews.all <- tm_map(reviews.all, stemDocument, language="english")

# Set up dtm
index.train <- c(1:320, 400 + 1:320)
train.dtm <- DocumentTermMatrix(reviews.all[index.train])
#train.dtm <- removeSparseTerms(train.dtm, 0.95)
test.dtm <- DocumentTermMatrix(reviews.all[-index.train],
list(dictionary=dimnames(train.dtm)[[2]]))
```

Load Multinomial Bayes function
```{r}
train.mnb <- function (dtm,labels) {
  call <- match.call()
  V <- ncol(dtm)
  N <- nrow(dtm)
  prior <- table(labels)/N
  labelnames <- names(prior)
  nclass <- length(prior)
  cond.probs <- matrix(nrow=V,ncol=nclass)
  dimnames(cond.probs)[[1]] <- dimnames(dtm)[[2]]
  dimnames(cond.probs)[[2]] <- labelnames
  index <- list(length=nclass)
  for(j in 1:nclass){
    index[[j]] <- c(1:N)[labels == labelnames[j]]
  }
  
  for(i in 1:V){
    for(j in 1:nclass){
      cond.probs[i,j] <- (sum(dtm[index[[j]],i])+1)/(sum(dtm[index[[j]],])+V)
    }
  }
  list(call=call,prior=prior,cond.probs=cond.probs)    
}

predict.mnb <- function (model,dtm) {
    classlabels <- dimnames(model$cond.probs)[[2]]
    logprobs <- dtm %*% log(model$cond.probs)
    N <- nrow(dtm)
    nclass <- ncol(model$cond.probs)
    logprobs <- logprobs+matrix(nrow=N,ncol=nclass,log(model$prior),byrow=T)
    classlabels[max.col(logprobs)]
}
```


logistic regression
```{r}
library(glmnet)
mod2 <- cv.glmnet(as.matrix(train.dtm),labels[index.train],
family="binomial",type.measure="class")
plot(mod2)
```

```{r, message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
library(randomForest)
# Unigrams
# Modelling

# Coding:
# mod1 / p1 = Naive Bayes
# mod2 / p2 = Regularized logistic regression 
# mod3 / p3 = Classification trees
# mod4 / p4 = Random forests
# (mod / p) * 2 = bigrams

mod1 <- train.mnb(as.matrix(train.dtm),labels[index.train])
mod3 <- rpart(classlabel~., data=data.frame(as.matrix(train.dtm),classlabel=labels[index.train]), method="class")
mod4 <- randomForest(label~., data=data.frame(as.matrix(train.dtm),label=labels[index.train]), mtry=sqrt(ncol(train.dtm)), ntree=100)

# Predicting
p1 <- predict.mnb(reviews.mnb,as.matrix(test.dtm))
# make predictions on the test set
p3<- predict(mod3,
newdata=data.frame(as.matrix(test.dtm)),type="class")
p4 <- predict(mod4, newdata = data.frame(as.matrix(test.dtm)), type = "class")
```

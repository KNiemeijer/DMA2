---
title: "DMA 2 V2"
output: html_notebook
---

# Load the text files into memory
```{r}
time.start <- Sys.time()
library(tm)
reviews.decep <- VCorpus(DirSource("C:/users/koenn/Google Drive/DM/Practical/DMA2/Reviews/deceptive", encoding="UTF-8", recursive=TRUE))
reviews.truth <- VCorpus(DirSource("C:/users/koenn/Google Drive/DM/Practical/DMA2/Reviews/truthful", encoding="UTF-8", recursive=TRUE))
reviews.all <- c(reviews.decep, reviews.truth)
labels <- c(rep(0,400),rep(1,400))
```

# Preprocessing
Preprocess the text corpus and create a document-term matrix
```{r}
# Clean all reviews
reviews.all <- tm_map(reviews.all,removePunctuation)
reviews.all <- tm_map(reviews.all,content_transformer(tolower))
reviews.all <- tm_map(reviews.all, removeWords,
stopwords("english"))
reviews.all <- tm_map(reviews.all,removeNumbers)
reviews.all <- tm_map(reviews.all,stripWhitespace)

# Set up dtm
index.train <- c(1:320, 400 + 1:320)
train.dtm <- DocumentTermMatrix(reviews.all[index.train])
test.dtm <- DocumentTermMatrix(reviews.all[-index.train],
list(dictionary=dimnames(train.dtm)[[2]]))

library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
train.bi.dtm <- DocumentTermMatrix(reviews.all[index.train], 
control = list(tokenize = BigramTokenizer))

train.bi <- cbind(as.matrix(train.dtm),as.matrix(train.bi.dtm)) # Merge unigrams and bigrams
test.bi.dtm <- DocumentTermMatrix(reviews.all[-index.train],
list(dictionary=dimnames(train.bi)[[2]]))
test.bi <- as.matrix(test.bi.dtm)
test.bi <- test.bi[,dimnames(train.bi)[[2]]]
```

First, we set up some functions we need to create these models.

```{r}
train.mnb <- function (dtm,labels) {
  call <- match.call()
  V <- ncol(dtm)
  N <- nrow(dtm)
  prior <- table(labels)/N
  labelnames <- names(prior)
  nclass <- length(prior)
  cond.probs <- matrix(nrow=V,ncol=nclass)
  dimnames(cond.probs)[[1]] <- dimnames(dtm)[[2]]
  dimnames(cond.probs)[[2]] <- labelnames
  index <- list(length=nclass)
  for(j in 1:nclass){
    index[[j]] <- c(1:N)[labels == labelnames[j]]
  }
  
  for(i in 1:V){
    for(j in 1:nclass){
      cond.probs[i,j] <- (sum(dtm[index[[j]],i])+1)/(sum(dtm[index[[j]],])+V)
    }
  }
  list(call=call,prior=prior,cond.probs=cond.probs)    
}

predict.mnb <- function (model,dtm) {
    classlabels <- dimnames(model$cond.probs)[[2]]
    logprobs <- dtm %*% log(model$cond.probs)
    N <- nrow(dtm)
    nclass <- ncol(model$cond.probs)
    logprobs <- logprobs+matrix(nrow=N,ncol=nclass,log(model$prior),byrow=T)
    classlabels[max.col(logprobs)]
}
```

```{r}
# Helper function for selecting lowest cp
cp.select <- function(big.tree) {
  min.x <- which.min(big.tree$cptable[, 4]) #column 4 is xerror
  for(i in 1:nrow(big.tree$cptable)) {
    if(big.tree$cptable[i, 4] < big.tree$cptable[min.x, 4] + big.tree$cptable[min.x, 5]) return(big.tree$cptable[i, 1]) #column 5: xstd, column 1: cp 
  }
}
```

To save on effort, we write a reusable function.

```{r}
createModels <- function(suffix, train, BiTrain, test, BiTest, labels) {
  library(rpart)
  library(rpart.plot)
  library(randomForest)
  library(glmnet)
  pred.cv <- list() # Set up list for predictions

  assign(paste0("mod1", suffix), train.mnb(as.matrix(train),labels),envir=.GlobalEnv)

  assign(paste0("mod2", suffix), cv.glmnet(as.matrix(train), labels, family="binomial",type.measure="class"), envir=.GlobalEnv)

  tempTree <- rpart(classlabel~., data=data.frame(as.matrix(train), classlabel=labels), cp=0, method="class")
  assign(paste0("mod3", suffix), prune(tempTree, cp=cp.select(tempTree)),envir=.GlobalEnv)

  assign(paste0("mod4", suffix), randomForest(classlabel~., data=data.frame(as.matrix(train),classlabel=labels), mtry=sqrt(ncol(train)), ntree=100),envir=.GlobalEnv)

  assign(paste0("mod5", suffix), train.mnb(as.matrix(BiTrain), labels),envir=.GlobalEnv)

  assign(paste0("mod6", suffix), cv.glmnet(as.matrix(BiTrain), labels, family="binomial",type.measure="class"),envir=.GlobalEnv)
   tempTree <- rpart(classlabel~.,data=data.frame(as.matrix(BiTrain),classlabel=labels), cp=0, method="class")

  assign(paste0("mod7", suffix),prune(tempTree, cp=cp.select(tempTree)),envir=.GlobalEnv)

  assign(paste0("mod8", suffix), randomForest(classlabel~., data=data.frame(as.matrix(BiTrain),classlabel=labels), mtry=sqrt(ncol(BiTrain)), ntree=100),envir=.GlobalEnv)
  
  # Prediction
  pred.cv[[1]] <- predict.mnb(get(paste0("mod1", suffix)), as.matrix(test))
  pred.cv[[2]] <- predict(get(paste0("mod2", suffix)), newx=as.matrix(test),s="lambda.1se",type="class")
  pred.cv[[3]] <- predict(get(paste0("mod3", suffix)), newdata=data.frame(as.matrix(test)),type="class")
  pTemp <- predict(get(paste0("mod4", suffix)), newdata = data.frame(as.matrix(test)), type = "class")
  pred.cv[[4]] <- as.numeric(pTemp > 0.5)
  pred.cv[[5]] <- predict.mnb(get(paste0("mod5", suffix)), as.matrix(BiTest))
  pred.cv[[6]] <- predict(get(paste0("mod6", suffix)), newx=as.matrix(BiTest), s="lambda.1se", type="class")
  pred.cv[[7]] <- predict(get(paste0("mod7", suffix)), newdata=data.frame(as.matrix(BiTest)), type="class")
  pTemp <- predict(get(paste0("mod8", suffix)), newdata = data.frame(as.matrix(BiTest)), type = "class")
  pred.cv[[8]] <- as.numeric(pTemp > 0.5)
  
  return(pred.cv)
}

``` 

Because we do not want to use all features (`dim(train.dtm)` is `[1]  640 6900`), we need to remove sparse features. That is, we remove features for the sparsity levels 0.90, 0.95, and 0.99. This means features that do not occur in, for instance, 99% of the documents get removed. We use 10-fold cross-validation to estimate the accuracy using the training set. We do not use the test set since we do not want to overfit on this test set which we'll need later. 

# Sparsity
```{r, warning=FALSE}
set.seed(37729)
k <- 10 #set the number of folds to 10
accvec.sparse <- matrix(nrow=8, ncol=3)
accvec.cv <- matrix(nrow=10, ncol=8)
folds <- ceiling(runif(train.dtm$nrow, min = 0, max = k))
accuracy <- function(obs, pred){
  sum(obs == pred) / length(obs)
}

sparsity <- c(0.90, 0.95, 0.99)

for(i in 1:length(sparsity)) {
  cat("\nCalculating Sparsity level ", sparsity[i])
  trainmat <- removeSparseTerms(train.dtm,sparsity[i])
  trainmat.bi <- removeSparseTerms(train.bi.dtm, sparsity[i])
  trainmat.bi <- cbind(as.matrix(trainmat),as.matrix(trainmat.bi))
  trainmat <- as.matrix(trainmat) 
  trainmat.bi <- as.matrix(trainmat.bi)
  
  for(j in 1:k) {
    pred.cv <- createModels(suffix=substr(as.character(sparsity[i]),2,4), train=trainmat[folds!=j,], BiTrain=trainmat.bi[folds!=j,], test=trainmat[folds==j, ], BiTest=trainmat.bi[folds==j,], labels=(labels[index.train])[folds!=j])
    
    for(n in 1:length(pred.cv)) 
      accvec.cv[j,n] <- accuracy(pred.cv[[n]], (labels[index.train])[folds==j])
  }
  accvec.sparse[,i] <- colMeans(accvec.cv)
}
rownames(accvec.sparse) <- c("NB", "Log reg", "Tree", "RF", "NB Bi", "Log reg Bi", "Tree Bi", "RF Bi")
colnames(accvec.sparse) <- c(sparsity)
accvec.sparse
cat("\nBest sparsity level is ", sparsity[which.max(colMeans(accvec.sparse))],
"with average accuracy = ", max(colMeans(accvec.sparse)))
library(ggplot2)
ggplot() + labs(x = "Sparsity level", y = "Accuracy") + scale_discrete_identity(as.factor(sparsity)) + geom_line(aes(sparsity, colMeans(accvec.sparse))) + geom_point(aes(sparsity,colMeans(accvec.sparse))) + geom_abline(colour="Red", slope=0, intercept=max(colMeans(accvec.sparse)))
```

# Stemming
Obviously, a sparsity level of 0.99 gives us the highest average accuracy over the models (0.81). We will use this level of sparsity for the rest of this research. 

Another interesting question we may pose is that whether we can improve performance by employing stemming. Similar to sparsity, we test this using 10-fold cross-validation and estimate its average accuracy on the training set. 

```{r, warning=FALSE}
# Set training data to correct sparsity level
train.dtm <- removeSparseTerms(train.dtm, 0.99)
test.dtm <- DocumentTermMatrix(reviews.all[-index.train],
list(dictionary=dimnames(train.dtm)[[2]]))

train.bi.dtm <- removeSparseTerms(train.bi.dtm, 0.99)
train.bi.dtm <- cbind(as.matrix(train.bi.dtm), as.matrix(train.dtm))
test.bi.dtm <- DocumentTermMatrix(reviews.all[-index.train],
list(dictionary=dimnames(train.bi.dtm)[[2]]))
test.bi.dtm <- as.matrix(test.bi.dtm)
test.bi.dtm <- test.bi.dtm[,dimnames(train.bi.dtm)[[2]]]

# Stem DocumentTermMatrices
reviews.all.stem <- tm_map(reviews.all, stemDocument, language="english")

train.stem <- DocumentTermMatrix(reviews.all.stem[index.train])
train.stem <- removeSparseTerms(train.stem, 0.99)
test.stem <- DocumentTermMatrix(reviews.all.stem[-index.train],
list(dictionary=dimnames(train.stem)[[2]]))

train.bi.stem <- DocumentTermMatrix(reviews.all.stem[index.train], 
control = list(tokenize = BigramTokenizer))
train.bi.stem <- removeSparseTerms(train.bi.stem, 0.99)
train.bi.stem <- cbind(as.matrix(train.stem), as.matrix(train.bi.stem))
test.bi.stem <- DocumentTermMatrix(reviews.all.stem[-index.train],
list(dictionary=dimnames(train.bi.stem)[[2]]))
test.bi.stem <- as.matrix(test.bi.stem)
test.bi.stem <- test.bi.stem[,dimnames(train.bi.stem)[[2]]]

set.seed(37729)
k <- 10 #set the number of folds to 10
accvec.stem <- matrix(nrow=8, ncol=2)
accvec.cv <- matrix(nrow=10, ncol=8)
folds <- ceiling(runif(train.dtm$nrow, min = 0, max = k))
accuracy <- function(obs, pred){
  sum(obs == pred) / length(obs)
}

compare <- list(train.dtm, train.bi.dtm, train.stem, train.bi.stem)
i <- 1

while(i <= 4) {
  if(ceiling(i/2)==1) { cat("\nCalculating Non-stemming") }
  else { cat("\nCalculating Stemming")}
  trainmat <- as.matrix(compare[[i]])
  trainmat.bi <- as.matrix(compare[[i+1]])
  
  for(j in 1:k) {
    pred.cv <- createModels(suffix=".stem", train=trainmat[folds!=j,], BiTrain=trainmat.bi[folds!=j,], test=trainmat[folds==j, ], BiTest=trainmat.bi[folds==j,], labels=(labels[index.train])[folds!=j])
    
    for(n in 1:length(pred.cv)) 
      accvec.cv[j,n] <- accuracy(pred.cv[[n]], (labels[index.train])[folds==j])
  }
  accvec.stem[,ceiling(i/2)] <- colMeans(accvec.cv)
  i <- i+2
}
rownames(accvec.stem) <- c("NB", "Log reg", "Tree", "RF", "NB Bi", "Log reg Bi", "Tree Bi", "RF Bi")
colnames(accvec.stem) <- c("No Stemming", "Stemming")
print(accvec.stem)
cat("\nThe best model is ", which.max(colMeans(accvec.stem)),
"(1 = no stemming, 2 = stemming) with average accuracy = ", max(colMeans(accvec.stem)))
accvec.stem
```

So, applying stemming is slightly better than when we don't apply stemming. A further improvement we might make is to look at the mutual information index of words. 

# Mutual Information
Mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the "amount of information" obtained about one random variable through observing the other random variable. We compare the top 200, top 100, top 50, and top 25 to see which one performs best using 10-fold CV. Although stemming did not prove to improve all methods, the differences are close and they also do not seem to hurt the improvement. Especially for logistic regression, stemming is an improvement and we will therefore use it when calculating mutual information. 

```{r, warning=FALSE}
# Apply stemming to data
# train.dtm <- train.stem
# train.bi.dtm <- train.bi.stem

set.seed(37729)
k <- 10 #set the number of folds to 10
accvec.mi <- matrix(nrow=8, ncol=7)
accvec.cv <- matrix(nrow=10, ncol=8)
folds <- ceiling(runif(train.stem$nrow, min = 0, max = k))
accuracy <- function(obs, pred){
  sum(obs == pred) / length(obs)
}

mi <- c(25, 50, 100, 200, 300, 400, 500)

library(entropy)
train.mi <- apply(as.matrix(train.stem), 2, function(x,y) { mi.plugin(table(x,y)/length(y)) }, labels[index.train])
train.mi.order <- order(train.mi,decreasing=T)
train.bi.mi <- apply(train.bi.stem, 2, function(x,y) { mi.plugin(table(x,y)/length(y)) }, labels[index.train])
train.bi.mi.stem.order <- order(train.bi.mi,decreasing=T)

for(i in 1:length(mi)) {
  cat("\nCalculating Mutual information top ", mi[i])

  for(j in 1:k) {
    pred.cv <- createModels(
      suffix=".mi", 
      train=(as.matrix(train.stem)[,train.mi.order[1:mi[i]]])[folds!=j,], 
      BiTrain=(train.bi.stem[,train.bi.mi.stem.order[1:mi[i]]])[folds!=j,], 
      test=(as.matrix(train.stem)[,train.mi.order[1:mi[i]]])[folds==j, ], 
      BiTest=(train.bi.stem[,train.bi.mi.stem.order[1:mi[i]]])[folds==j,], 
      labels=(labels[index.train])[folds!=j]
      )
    
    for(n in 1:length(pred.cv)) 
      accvec.cv[j,n] <- accuracy(pred.cv[[n]], (labels[index.train])[folds==j])
  }
  accvec.mi[,i] <- colMeans(accvec.cv)
}
rownames(accvec.mi) <- c("NB", "Log reg", "Tree", "RF", "NB Bi", "Log reg Bi", "Tree Bi", "RF Bi")
colnames(accvec.mi) <- c(mi)
accvec.mi
cat("Best mutual information level is top", mi[which.max(colMeans(accvec.mi))],
"with average accuracy = ", max(colMeans(accvec.mi)))
ggplot() + labs(x = "Number of features", y = "Accuracy") + scale_discrete_identity(as.factor(mi)) + geom_line(aes(mi, colMeans(accvec.mi))) + geom_point(aes(mi,colMeans(accvec.mi))) + geom_abline(colour="Red", slope=0, intercept=max(colMeans(accvec.mi)))
```

It looks like for most features using the top 500 features based on mutual information provides the most improvement. That said, the slope of the line flattens out after 300 features and more than 500 is not likely to create more accurate models. We try one more possible improvement before comparing the results to check which algorithms work best with which feature selection.

# TF-IDF
```{r, warning=FALSE}
set.seed(37729)
k <- 10 #set the number of folds to 10
accvec.tf <- matrix(nrow=8, ncol=1)
accvec.cv <- matrix(nrow=10, ncol=8)
folds <- ceiling(runif(train.stem$nrow, min = 0, max = k))
accuracy <- function(obs, pred){
  sum(obs == pred) / length(obs)
}

train.tf <- DocumentTermMatrix(reviews.all.stem[index.train], 
control=list(weighting=weightTfIdf)) # Use stemmed data due to size differences with test set
train.tf <- removeSparseTerms(train.tf,0.99) # Use the sparsity level as selected before
train.mat <- as.matrix(train.stem)
nRows <- nrow(train.mat)
nCols <- ncol(train.mat)
train.mat <- matrix(as.numeric(train.mat > 0),nrow=nRows,ncol=nCols)
train.idf <- apply(train.mat,2,sum)
train.idf <- log2(nRows/train.idf)
test.tf <- as.matrix(train.stem) # Use training data instead of test data
for(i in 1:1114){test.tf[,i] <- test.tf[,i]*train.idf[i]}

train.bi.tf <-  DocumentTermMatrix(reviews.all.stem[index.train], 
control=list(weighting=weightTfIdf, tokenize = BigramTokenizer))
train.bi.tf <- removeSparseTerms(train.bi.tf,0.99)
train.bi.mat <- as.matrix(train.bi.stem)
nRows <- nrow(train.bi.mat)
nCols <- ncol(train.bi.mat)
train.bi.mat <- matrix(as.numeric(train.bi.mat > 0),nrow=nRows,ncol=nCols)
train.bi.idf <- apply(train.bi.mat,2,sum)
train.bi.idf <- log2(nRows/train.bi.idf)
test.bi.tf <- as.matrix(train.bi.stem) # Use training instead of test data
for(i in 1:1575){test.bi.tf[,i] <- test.bi.tf[,i]*train.bi.idf[i]}

for(j in 1:k) {
   pred.cv <- createModels(suffix=".tf", train=train.tf[folds!=j,], BiTrain=train.bi.tf[folds!=j,], test=test.tf[folds==j,], BiTest=train.bi.tf[folds==j,], labels=(labels[index.train])[folds!=j])
  
  for(n in 1:length(pred.cv)) 
    accvec.cv[j,n] <- accuracy(pred.cv[[n]], (labels[index.train])[folds==j])
}
accvec.tf[,1] <- colMeans(accvec.cv)

rownames(accvec.tf) <- c("NB", "Log reg", "Tree", "RF", "NB Bi", "Log reg Bi", "Tree Bi", "RF Bi")
colnames(accvec.tf) <- "TF-IDF"
accvec.tf
cat("Average accuracy = ", max(colMeans(accvec.tf)))
```

Okay that isn't much of an improvement. Now that we have calculated different forms of feature selection, we do a side-by-side comparision and select the best of of feature selection applicable to a specific model before validating this model on the test set.

# Comparison of Feature Selection
```{r}
mod.mat <- cbind(accvec.sparse, accvec.stem, accvec.mi, accvec.tf)
for(i in 1:nrow(mod.mat)) {
  cat("\nAccuracy for ", rownames(mod.mat)[i], "\n")
  print(as.matrix((mod.mat[i,])[order(mod.mat[i,], decreasing=T)]))
}
```

Based on these results, we can draw the follow inferences:
* Naive Bayes: Non-stemming gives an improvement over stemming. Additonally, the top-300 features based on mutual information give the higher accuracy of 0.8880763. We will combine these two to see if this indeed performs better.
* Logistic regression: As concluded before, stemming seems to give a small improvement in comparison to non-stemmed features. The top-200 features based on mutual information give the higher accuracy of 0.8702152.
* Decision tree: Interestingly, a single decision tree seems to work abysmally in comparison to the other methods. Also unlike the other methods, a decision tree seems to work best when either all the non-stemmed features are used or when only when the top-25 features are used based on mutual information. We will combine these two just to be sure.
* Random forests: Fortunately, random forests perform much better than just a single tree. The top-500 features with stemming seems most promising and so we will put this to the test in the next session. 
* Naive Bayes with uni- and bigrams: The top-300 features seem to hit the mark when using NB. Since non-stemmed features perform slightly better, we will have to see if a combination of the two provides an improvement.
* Logistic regression with uni- and bigrams: Similar to logistic regression with only unigrams, this algorithm with bigrams performs best with stemmed features. This time the top 300 is used instead.
* Decision tree with uni- and bigrams: The results for this algorithm is inconclusive. On the one hand, features with a sparsity level of $<0.99$ seem to performed best, but this should of course be equivalent to the results of 'no stemming' since no stemming was used when calculating the sparsity levels. However, the results also show that stemming performs better than no stemming. Consequently, we will validate these results in the next section by using both stemming and a sparsity level of 0.99.
* Random forests with uni- and bigrams: Similar to random forests with only unigrams, the top-500 features with stemming seems most promising.

Now we make a comparison after which we select our final models. Since we have already created the models for decision trees, logistic regression, random forests, random forests with uni- and bigrams, and logistic regression with uni- and bigrams, we do not need to train these again.

```{r}
best.mat <- matrix(ncol=1, nrow=8)
for(i in 1:nrow(mod.mat)) {
  best.mat[i,] <- max(mod.mat[i,])
}
newbest <- numeric(8)
newbest[c(2,4,6,7,8)] <- best.mat[c(2,4,6,7,8),1]

# First, we set up the data we need

# Tree bi: sparsity 0.99, stem -> No need to train
# Log: mi 200, stem -> No need to train!
# RF: mi 500, stem
# RF bi: mi 500, stem
# Log bi: mi 300, stem

# Now for non-stemmed data
# library(RWeka)
# train.dtm <- DocumentTermMatrix(reviews.all[index.train])
# train.bi.dtm <- DocumentTermMatrix(reviews.all[index.train], 
# control = list(tokenize = BigramTokenizer))
# train.bi <- cbind(as.matrix(train.dtm),as.matrix(train.bi.dtm)) # Merge unigrams and bigrams

library(entropy)
train.mi.nostem <- apply(as.matrix(train.dtm), 2, function(x,y) { mi.plugin(table(x,y)/length(y)) }, labels[index.train])
train.mi.nostem.order <- order(train.mi.nostem,decreasing=T)
train.bi.mi <- apply(train.bi, 2, function(x,y) { mi.plugin(table(x,y)/length(y)) }, labels[index.train])
train.bi.mi.order <- order(train.bi.mi,decreasing=T)

set.seed(37729)
k <- 10 #set the number of folds to 10
accvec.bs <- rep(0, k)
folds <- ceiling(runif(nrow(train.dtm), min = 0, max = k))
accuracy <- function(obs, pred){
  sum(obs == pred) / length(obs)
}
# Tree: mi 25, non-stem
for(j in 1:k) {
  tempTree <- rpart(classlabel~., data=data.frame((as.matrix(train.dtm)[,train.mi.nostem.order[1:20]])[folds!=j,], classlabel=(labels[index.train])[folds!=j]), cp=0, method="class") #train on all but fold j
  mod <- prune(tempTree, cp=cp.select(tempTree)) # Select best CP
  pred <- predict(mod, newdata = data.frame((as.matrix(train.dtm)[,train.mi.nostem.order[1:20]])[folds==j,]), type="class") #validate on fold j
  accvec.bs[j] <- accuracy((labels[index.train])[folds==j], pred)
}
newbest[3] <- mean(accvec.bs)

# NB: mi 300, non-stem
for(j in 1:k) {
  mod<- train.mnb((as.matrix(train.dtm)[,train.mi.nostem.order[1:300]])[folds!=j,], (labels[index.train])[folds!=j]) #train on all but fold j 
  pred <- predict.mnb(mod, (as.matrix(train.dtm)[,train.mi.nostem.order[1:300]])[folds==j,]) #validate on fold j
  accvec.bs[j] <- accuracy((labels[index.train])[folds==j], pred)
}
newbest[1] <- mean(accvec.bs)

# NB bi: mi 300, non-stem
for(j in 1:k) {
  mod<- train.mnb((train.bi[,train.bi.mi.order[1:300]])[folds!=j,], (labels[index.train])[folds!=j]) #train on all but fold j 
  pred <- predict.mnb(mod, (train.bi[,train.bi.mi.order[1:300]])[folds==j,]) #validate on fold j
  accvec.bs[j] <- accuracy((labels[index.train])[folds==j], pred)
}
newbest[5] <- mean(accvec.bs)

best.mat <- cbind(best.mat, newbest)
rownames(best.mat) <- rownames(mod.mat)
colnames(best.mat) <- c("Old Accuracy", "New Accuracy")
```

The results for a single decision tree are disappointing. Interestingly, Naive Bayes with uni- and bigrams seems to still score slightly below its previous highest accuracy for which it used the top-300 stemmed features based on mutual information. The only model that has improved is naive with unigrams. Therefore, we will use this new model.

# Final Testing of the Models
```{r}
# mod1 mi 300 no stem
# mod2 mi 200 stem
# mod3 no stemming
# mod4 mi 500 stem
# mod5 mi 300 stem
# mod6 mi 300 stem
# mod7 sparse no stem
# mod8 mi 500 stem

mod1 <- train.mnb(as.matrix(train.dtm)[,train.mi.nostem.order[1:300]], labels[index.train])
mod2 <- cv.glmnet(as.matrix(train.stem)[,train.mi.order[1:200]], labels[index.train], alpha=1, family="binomial",type.measure="class")
tempTree <- rpart(classlabel~., data=data.frame(as.matrix(train.dtm), classlabel=labels[index.train]), cp=0, method="class")
mod3 <- prune(tempTree, cp=cp.select(tempTree))
mod4 <- randomForest(classlabel~., data=data.frame(as.matrix(train.stem)[,train.mi.order[1:500]], classlabel=labels[index.train]), mtry=sqrt(ncol(train.stem)), ntree=100)
mod5 <- train.mnb(as.matrix(train.bi.stem), labels[index.train])
mod6 <- cv.glmnet(as.matrix(train.bi.stem)[,train.bi.mi.stem.order[1:300]], labels[index.train], alpha=1, family="binomial", type.measure="class")
tempTree <- rpart(classlabel~., data=data.frame(as.matrix(train.bi.dtm), classlabel=labels[index.train]), cp=0, method="class")
mod7 <- prune(tempTree, cp=cp.select(tempTree))
mod8 <- randomForest(classlabel~., data=data.frame(as.matrix(train.bi.stem)[,train.bi.mi.stem.order[1:500]], classlabel=labels[index.train]), mtry=sqrt(ncol(train.bi.stem)), ntree=100)

# TO DO: Change test sets
p1 <- predict.mnb(mod1, as.matrix(test.dtm)[,train.mi.nostem.order[1:300]])
p2 <- predict(mod2, newx=as.matrix(test.stem)[,train.mi.order[1:200]], s="lambda.1se",type="class")
p3 <- predict(mod3, newdata=data.frame(as.matrix(test.dtm)),type="class")
pTemp <- predict(mod4, newdata = data.frame(as.matrix(test.stem)[,train.mi.order[1:500]]), type="class")
p4 <- as.numeric(pTemp > 0.5)
p5 <- predict.mnb(mod5, as.matrix(test.bi.stem))
p6 <- predict(mod6, newx=as.matrix(test.bi.stem)[,train.bi.mi.stem.order[1:300]], s="lambda.1se", type="class")
p7 <- predict(mod7, newdata=data.frame(as.matrix(test.bi.dtm)), type="class")
pTemp <- predict(mod8, newdata = data.frame(as.matrix(test.bi.stem)[,train.bi.mi.stem.order[1:500]]), type="class")
p8 <- as.numeric(pTemp > 0.5)

newbest <- numeric(8)
for(i in 1:8)
  newbest[i] <- accuracy(labels[-index.train], get(paste0("p",i)))
best.mat <- cbind(best.mat, newbest)
colnames(best.mat) <- c("Old Accuracy", "New Accuracy", "Final Accuracy")
time.stop <- Sys.time()
```
